Cell 0:
# Lab Session - Basic ML Concepts: Probability Distributions and Information Theory
---

In this lab session, we are going to consider various probability distributions, conditional probability and information theory.

__Note__: You can complete the lab entering code only in the  __# EXPECTED SPACE FOR STUDENT CODE__-blocks as well as the empty cells.
-----
Cell 2:

# Exercise 1: Gaussian Distributions

We will start by considering the most prevalent distribution - the Gaussian. The Gaussian has several appealing properties that makes it easy to work with. In the following tasks, we will highlight some of these.

**Note:** For the following tasks, the purpose is to understand the properties of Gaussian distributions. As such, any readly-made distributions (scipy.stats.norm etc.) are disallowed. Using NumPy or SciPy for Linear Algebra is permitted.

Below, you see an implementation of a Gaussian distribution. We will use this highlight and implement properties that are typical of the Gaussian distribution. Please read through, and note the dunder (double under, \_\_methodname_\__) methods that are implemented. These allow us to add and subtract one Gaussian with another.
-----
Cell 4:
### Exercise 1.1
We first want to demonstrate how a Gaussian looks and how it is expected to behave - particularly its linear properties. __Throughout this task, we assume that the random variables we work with are independent of one another__ (e.g. __A__ is independent of __B__). Create four Gaussians:

__A__: $\mu = 1, \sigma^2 = 2$

__B__: $\mu = -2, \sigma^2 = 0.1$

__C__: $\mu = \begin{bmatrix}
1 & 0
\end{bmatrix}, \quad\Sigma^2 = \begin{bmatrix}
1 & 0.9\\
0.9 & 2
\end{bmatrix}$

__D__: $\mu = \begin{bmatrix}
-1 & 2
\end{bmatrix}, \quad\Sigma^2 = \begin{bmatrix}
1 & -0.7\\
-0.7 & 1
\end{bmatrix}$

Visualize, through the provided plotting function below, _plot_gaussian_:

a. __A__

b. __B__

c. __A__ + __B__

d. __C__

e. __D__

f. __C__ - __D__

__Questions__:

1. What is meant by "The Gaussian distribution is fully characterized by its mean and covariance"?
2. If two Gaussians are added, can the (co-)variance ever decrease?
-----
Cell 5:
__Answers__:

1.
2.
-----
Cell 7:
### Exercise 1.2
__Task__: Implement the `marginalize` function of the class GaussianDistribution. Test, for __C__, that your resulting Gaussian looks like the one in the plot. Plot the resulting distribution for __C__ when marginalizing on $y$, and on __D__ when marginalizing on $x$.

__Question__: What does one effectively do when marginalizing a variable of a distribution?
-----
Cell 8:
__Answer__:

-----
Cell 11:
### Exercise 1.3
__Task__: Implement the `get_conditional` function of the class GaussianDistribution. Test, for the Gaussians __C__ and __D__ from 1.1, that your resulting conditional looks like the ones in the plots, when conditioning on $x=2$ and $y=4$, respectively. 

__Hint 1__: Use the numpy arrays to retrieve the right dimensions when performing the computation (`cond_dims` are the dimensions we condition on, `non_cond_dims` are the non-conditioned dimensions). Example:

$\Sigma_{yx} = $ `self.cov[cond_dims, non_cond_dims][:, np.newaxis]`

__Hint 2__: Use either np.matmul, np.dot or the @ operator to perform matrix matrix multiplications.
-----
Cell 14:
# Exercise 2: Information Theory

Here, we will work with entropy, information gain and mutual information.
-----
Cell 15:
### Exercise 2.1
In this exercise we are going to work with characters in an alphabet.
Assume, you have a string 'cdd', then the frequencies of 'c' and 'd' are $ p(^\prime{}c^\prime{})=\frac{1}{3} $ and $ p(^\prime{}d^\prime{})=\frac{2}{3} $.
We can interpret $ p $ as a probability distribution over an alphabet.

Assume you only have two characters in an alphabet, 'a' and 'b'. 


* **What does the probability of 'a' and 'b' have to be in order for the entropy to be maximal?**
* **What is the maximum value of the entropy in this case?**
* **Intuitively, why is the entropy maximum for the value you found?** (You can calculate it on paper or write it in Python, you will need it later anyway)
-----
Cell 16:
***Your answers:***
-----
Cell 17:
### Exercise 2.2
-----
Cell 18:
Still assume that your alphabet consists of only two characters, 'a' and 'b'.
For values $ p(^\prime{}a^\prime{}) = 0.01, 0.02, \ldots, 0.99 $, plot the entropy of your alphabet ($ x $-axis: $ p(^\prime{}a^\prime{}) $, $ y $-axis: Entropy in bits).
Your plot should contain only one line.

If you don't know how to plot with Matplotlib, there is a really short crash course in the next cell.
-----
Cell 21:
### Exercise 2.3
A measure that is related to Entropy is the *Gini Impurity* which is defined as
$$
    G(p) = 1-\sum_{x\in\mathcal{X}}p(x)^2
$$

Both entropy and Gini impurity are used in decision tree learning to measure the ``level of disorderliness''. In the following, we want to discover similarities and differences.

Extend the plot from Exercise 2.2, such that you also plot the Gini Impurity for the different values of $ p(^\prime{} a^\prime{}) $. **What do you notice?**
-----
Cell 23:
Now we extend our comparison to three dimensions. We have three variables $ x_1 $, $ x_2 $, and $ x_3 $ and plot their entropies and Gini impurities.
The $ x $- and $ y $-axes show the values $ p(x_1) $ and $ p(x_2) $, respectively. The value $ p(x_3) $ is implicitly defined as $ 1-p(x_2)-p(x_1) $.
We also plot the point of maximum entropy / gini impurity. 

**What do you notice? If you have $ n $ instead of three random variables, what are the bounds for the entropy and the Gini impurity?**
-----
Cell 25:
# Exercise 3
-----
Cell 26:
In the next exercise, we will work with joint probability distributions. For this purpose, we use the Kaggle Titanic dataset which consists of records of survivors and non-survivors of the Titanic disaster. For this exercise, we will be only interested in some of the attributes, `Survived` and `Sex`.
-----
Cell 28:
For example, we can find the male passengers as follows
-----
Cell 30:
and we can find the number male passengers by
-----
Cell 32:
Similarly, we can filter by multiple attributes, for example, to find all male passenger in the third class
-----
Cell 34:
We can select only the passenger names by
-----
Cell 36:
### Exercise 3.1
-----
Cell 37:
***Fill in the following table.*** 

The random variable $ S $ describes a passenger's sex, the variable $ V $ whether they survived or not.


| $ S $ / $ V $ | dead | alive | P($ S $) |
| --------- | ---- | ----- | ------ |
| **female** | ??? | ??? | ??? |
| **male** | ??? | ??? | ??? |
| **P($ V $)** | ??? | ??? | 1 |
-----
Cell 38:
### Exercise 3.2

* **Find the value $ H(S) $. Is this a high or a low entropy? For what distribution would the uncertainty be maximal/minimal (with which value)?**
-----
Cell 40:
### Exercise 3.3

It is commonly said that the class you traveled in affected your survival chances. Let's test this.

* **Calculate $ H(V) $, $ H(V|C) $, and $ MI(V; C) $** where $ C $ is an RV for the class ('Pclass' in the dataset). Does knowing the class you traveled in reduce your uncertainty about whether a passenger died?
-----
Cell 42:
### Exercise 3.4

Now do the same for $ H(V|S) $ and $ MI(V; S) $.

* **Which does help you more in deciding whether a passenger died or not, knowing their sex or travel class?**
-----
Cell 44:
# Exercise 4 - MLE, Overfitting and Cross-validation
Here, we are going to look at a synthetic regression problem, and how under- and overfitting can occur through better or worse model parameterizations. We will also look at the most common mean of evaluating model performance, namely cross-validation. We'll consider learning the function below through polynomial regression on a number of given data points:
-----
Cell 46:
### Exercise 4.1
Look at the class `PolynomialRegression` (a type of Generalized Linear Model) and pay attention to its attributes and _predict_ method. This class is intended to fit a polynomial of a given degree to the data points, by calling some method to fit the data points.

__Task__: Implement the closed-form Maximum Likelihood parameter estimation in the `fit_MLE` method. The closed-form expression for the optimal parameters can be found in the Lecture 3 on the section on Maximum Likelihood. Ensure it works by fitting a polynomial of degree 10 to the data points - it should fit most data points reasonably well.

__Hint__: Keep in mind that a polynomial of degree 2 has 3 coefficients.
-----
Cell 49:
### Exercise 4.2
We will now discover what happens if we under- and overfit parameters, as well as how additional data helps counteract overfitting. 

__Task__: Toy around with the fit of the generalized linear model below to find degrees where it under- and overfits. Try polynomials of different orders by changing the _polynomial_degree_ parameter. When you overfit, try change the _added_samples_ parameter.


__Questions__:

1. Judging by eye, which degree seems to consistently give the best fit?
2. How can one tell, only by looking at the parameter values of the model, if overfitting has occured?
-----
Cell 50:
__Answer__:

1.
2.
-----
Cell 53:
### Exercise 4.3
To evaluate the models produced, one commonly uses cross-validation. 

__Task__: Implement K-fold cross validation and find your best model with RMSE as your metric. Plot the best model. Use only the initial data, without additional samples. Implement yourself or use an existing tool, e.g.  <a href="http://scikit-learn.org/stable/modules/cross_validation.html" title="Scikit-Learn Cross Validation">Scikit-Learn</a>.

__Tip__: Ensure nearby points don't go in the same batch, so that every model has data from the entire space.

__Question__:
What performance (RMSE, average on the test sets) did your top-performing model yield for K = 5?
-----
Cell 54:
__Answer__:
-----
Cell 56:
### Exercise 4.4 (Optional)

You might have noticed that when you set the polynomial degree higher than the number of data points, the polynomial does not fit the data perfectly. However, this is what you would expect as the linear system of equations is overconstrained (test this by, e.g., using a polynomial of degree 100):
-----
Cell 58:
Below is the closed-form MLE solution which you (probably) used to solve above question. However, note that our original problem is a least squares problem:

$$
   \min_{\mathbf{\beta}} || \mathbf{y} - \theta\beta  ||^{2}
$$

You might have noticed that for high polynomial degrees, the polynomial does not fit the datapoints perfectly - even though this is what you would expect. This is because using the closed-form solution is numerically unstable. In fact, we are better off using NumPy's `numpy.linalg.lstsq` which takes the design matrix $\theta$ and the vector $\mathbf{y}$ as input and returns a $\beta$ that satisfies above formula. `numpy.linalg.lstsq` uses an iterative approach for solving the problem and is much more stable.

**Task:** Change the `fit_MLE` function to use `numpy.linalg.lstsq` and see how the solution gets more stable by using polynomials of high degrees. Note that `numpy.linalg.lstsq` returns a tuple from which you need the first element, also you might want to set `rcond=None` when calling `numpy.linalg.lstsq` to suppress warnings.

After changing the `fit_MLE` function, the polynomial of degree 100 should fit all points perfectly.
-----
